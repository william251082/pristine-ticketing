kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission

Accessing Env Variables in a Pod
imperative approach, instead of writing a config file(declarative)
kubectl create secret generic jwt-secret --from-literal=JWT_KEY=asdf
kubectl get secrets
kubectl describe pods <name_of_pod>

File detection change
kubectl get pods
kubectl delete pod client-depl-85b6fb44f9-2wrph
kubectl get pods


I found out that minikube has it own docker instance

Run eval $(minikube docker-env)

Then in same terminal window run docker system df

This command will print out something like this

TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              5                   2                   16.43 MB            11.63 MB (70%)
Containers          2                   0                   212 B               212 B (100%)
Local Volumes       2                   1                   36 B                0 B (0%)
Based on this information you need to clear up resources

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ npm install node-nats-streaming ts-node-dev typescript @types/node


kubectl get pods
kubectl port-forward nats-depl-78677c4455-8nqqb 4222:4222
on another terminal window:
npm run publish

npm run publish
npm run listen
rs to restart

Queue groups
-- making sure the published event is not duplicated among every service listeners


default --when service received an event, it will automatically marked as event received, leading to error data (if any), being lost
overwrite default behavior using .setManualAckMode(true);

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl port-forward nats-depl-78677c4455-8nqqb 8222:8222
on browser: localhost:8222/streaming
localhost:8222/streaming/channelsz?subs=1
-- help nats to understand to know a client subscription that won't ever come back
-hb -- request that nats is going to send to all connected clients every second
-hbi -- how often nats will send requests to the connected clients
-hbt -- how long each client has to respond
-hbt -- number of times the request should fail before nats will assume that the connection is gone

setDeliverAllAvailable()
restart npm run publish
npm run listen
list of all events will be listed down

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/common$ npm install node-nats-streaming
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/common$ npm run pub
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/tickets$ npm update @iceshoptickets/common

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl delete pod nats-depl-78677c4455-8nqqb


iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl delete pod nats-depl-85f7b46dfb-hr2d6
pod "nats-depl-85f7b46dfb-hr2d6" deleted
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl get pods
NAME                                  READY   STATUS    RESTARTS   AGE
auth-depl-7b69cc7d47-8msxf            1/1     Running   0          7m26s
auth-mongo-depl-fc4596ff8-k4lzs       1/1     Running   0          7m26s
client-depl-54fffbfc8b-spdwv          1/1     Running   0          7m26s
nats-depl-85f7b46dfb-qtgcs            1/1     Running   0          17s
tickets-depl-7f8746c7b-slznr          1/1     Running   1          2m
tickets-mongo-depl-68cbb6cd78-5xr9n   1/1     Running   0          7m26s
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ kubectl port-forward nats-depl-85f7b46dfb-qtgcs 4222:4222
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ npm run listen
on post man post a new ticket https://ticketing.dev/api/tickets

Handling Publish Failures
-- saving the record and event at the same time
-- separate code/process watching Events
--pull events from events collection the publish to nats
-- wrap up saving the event inside the database transaction


