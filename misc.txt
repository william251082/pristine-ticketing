kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission

Accessing Env Variables in a Pod
imperative approach, instead of writing a config file(declarative)
kubectl create secret generic jwt-secret --from-literal=JWT_KEY=asdf
kubectl get secrets
kubectl describe pods <name_of_pod>

File detection change
kubectl get pods
kubectl delete pod client-depl-85b6fb44f9-2wrph
kubectl get pods


I found out that minikube has it own docker instance

Run eval $(minikube docker-env)

Then in same terminal window run docker system df

This command will print out something like this

TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              5                   2                   16.43 MB            11.63 MB (70%)
Containers          2                   0                   212 B               212 B (100%)
Local Volumes       2                   1                   36 B                0 B (0%)
Based on this information you need to clear up resources

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ npm install node-nats-streaming ts-node-dev typescript @types/node


kubectl get pods
kubectl port-forward nats-depl-78677c4455-8nqqb 4222:4222
on another terminal window:
npm run publish

npm run publish
npm run listen
rs to restart

Queue groups
-- making sure the published event is not duplicated among every service listeners


default --when service received an event, it will automatically marked as event received, leading to error data (if any), being lost
overwrite default behavior using .setManualAckMode(true);

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl port-forward nats-depl-78677c4455-8nqqb 8222:8222
on browser: localhost:8222/streaming
localhost:8222/streaming/channelsz?subs=1
-- help nats to understand to know a client subscription that won't ever come back
-hb -- request that nats is going to send to all connected clients every second
-hbi -- how often nats will send requests to the connected clients
-hbt -- how long each client has to respond
-hbt -- number of times the request should fail before nats will assume that the connection is gone

setDeliverAllAvailable()
restart npm run publish
npm run listen
list of all events will be listed down

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/common$ npm install node-nats-streaming
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/common$ npm run pub
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/tickets$ npm update @iceshoptickets/common

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl delete pod nats-depl-78677c4455-8nqqb


iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl delete pod nats-depl-85f7b46dfb-hr2d6
pod "nats-depl-85f7b46dfb-hr2d6" deleted
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl get pods
NAME                                  READY   STATUS    RESTARTS   AGE
auth-depl-7b69cc7d47-8msxf            1/1     Running   0          7m26s
auth-mongo-depl-fc4596ff8-k4lzs       1/1     Running   0          7m26s
client-depl-54fffbfc8b-spdwv          1/1     Running   0          7m26s
nats-depl-85f7b46dfb-qtgcs            1/1     Running   0          17s
tickets-depl-7f8746c7b-slznr          1/1     Running   1          2m
tickets-mongo-depl-68cbb6cd78-5xr9n   1/1     Running   0          7m26s
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ kubectl port-forward nats-depl-85f7b46dfb-qtgcs 4222:4222
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/nats-test$ npm run listen
on post man post a new ticket https://ticketing.dev/api/tickets

Handling Publish Failures
-- saving the record and event at the same time
-- separate code/process watching Events
--pull events from events collection the publish to nats
-- wrap up saving the event inside the database transaction

kubectl get pods


iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/orders$ npm install
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/orders$ docker build -t iceshop/orders .
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/orders$ docker push iceshop/orders

A Quick Manual Test

POST https://ticketing.dev/api/tickets
{
    "title": "concert",
    "price": 80
}

PUT https://ticketing.dev/api/tickets/5f23c9371110780023f2475f
{
    "title": "concert",
    "price": 81
}

creating ticket
[tickets-depl-dd4847768-9b4br tickets] Event published to subject ticket:created
[orders-depl-784bd94794-mf2sm orders] Message received: ticket:created / orders-service

updating ticket
[tickets-depl-dd4847768-9b4br tickets] Event published to subject ticket:updated
[orders-depl-784bd94794-mf2sm orders] Message received: ticket:updated / orders-service


MONGO SHELL TO THE ORDERS DB
db.tickets.remove({})
db.tickets.find({price:10}).length()

MONGO SHELL TO THE TICKETS DB
db.tickets.remove({})
db.tickets.find({price:10}).length()

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/tickets$ npm install mongoose-update-if-current
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/orders$ npm install mongoose-update-if-current


with version attribute
creating ticket
[tickets-depl-dd4847768-9b4br tickets] Event published to subject ticket:created
[orders-depl-784bd94794-mf2sm orders] Message received: ticket:created / orders-service

updating ticket
[tickets-depl-dd4847768-9b4br tickets] Event published to subject ticket:updated
[orders-depl-784bd94794-mf2sm orders] Message received: ticket:updated / orders-service


executing mongo db in a pod
kubectl get pods
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/tickets$ kubectl exec -it tickets-mongo-depl-57dfb6bb6b-7zbpb mongo
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/orders$ kubectl exec -it orders-mongo-depl-57dfb6bb6b-7zbpb mongo

> show dbs;
admin   0.000GB
config  0.000GB
local   0.000GB
orders  0.000GB
> use orders;
switched to db orders
> db.tickets
orders.tickets
> db.tickets.find({ price: 80 })
> db.tickets.find({ price: 8000 })
> db.tickets.find({ price: 800 })

whenever there's order:cancelled or order:create, event should be emitted
so all services can update their own versions


npm install bull @types/bull
docker build -t iceshop/expiration .
docker push iceshop/expiration

Manually restart pod:
kubectl delete pod <pod_name>

Testing Job Processing
on postman
GET https://ticketing.dev/api/users/currentuser
POST https://ticketing.dev/api/tickets
{
    "title": "concert",
    "price": 908
}
POST https://ticketing.dev/api/orders
{
    "ticketId": "5f267de95757af0018ddf45b"
}

[tickets-depl-7b8c654ffb-fmvt6 tickets] Event published to subject ticket:created
[orders-depl-5755694bf7-9v7sx orders] Message received: ticket:created / orders-service
[orders-depl-5755694bf7-9v7sx orders] Event published to subject order:created
[tickets-depl-7b8c654ffb-fmvt6 tickets] Message received: order:created / tickets-service
[expiration-depl-56b4bd57b9-2nthb expiration] Message received: order:created / expiration-service
[expiration-depl-56b4bd57b9-2nthb expiration] Waiting this many milliseconds to process the job:  59983
[orders-depl-5755694bf7-9v7sx orders] Message received: ticket:updated / orders-service
[tickets-depl-7b8c654ffb-fmvt6 tickets] Event published to subject ticket:updated


restart minikube:
1. minikube delete
2. minikube start
3. kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud/deploy.yaml
4. Set the JWT as a secret
    kubectl create secret generic jwt-secret --from-literal=JWT_KEY=asdf
    kubectl get secrets
    kubectl describe pods <name_of_pod>
    kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission
5. kubectl create secret generic stripe-secret --from-literal STRIPE_KEY=<stripe_secret_key>
    kubectl get secrets
6. skaffold dev

TODO:
[tickets-depl-699dd5879-4kwbm tickets] Event published to subject ticket:created
[orders-depl-8c8cd596c-4qgm2 orders] Message received: ticket:created / orders-service
[tickets-depl-699dd5879-4kwbm tickets] Message received: order:created / tickets-service
[expiration-depl-b57874dd4-htckm expiration] Message received: order:created / expiration-service
[expiration-depl-b57874dd4-htckm expiration] Waiting this many milliseconds to process the job:  59994
[orders-depl-8c8cd596c-4qgm2 orders] Event published to subject order:created
[tickets-depl-699dd5879-4kwbm tickets] Event published to subject ticket:updated
[orders-depl-8c8cd596c-4qgm2 orders] Message received: ticket:updated / orders-service
[expiration-depl-b57874dd4-htckm expiration] (node:24) UnhandledPromiseRejectionWarning: Error: stan: publish ack timeout
[expiration-depl-b57874dd4-htckm expiration]     at Timeout.<anonymous> (/app/node_modules/node-nats-streaming/lib/stan.js:603:18)
[expiration-depl-b57874dd4-htckm expiration]     at listOnTimeout (internal/timers.js:551:17)
[expiration-depl-b57874dd4-htckm expiration]     at processTimers (internal/timers.js:494:7)
[expiration-depl-b57874dd4-htckm expiration] (Use `node --trace-warnings ...` to show where the warning was created)
[expiration-depl-b57874dd4-htckm expiration] (node:24) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)
[expiration-depl-b57874dd4-htckm expiration] (node:24) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.

TODO: DONE
[orders-depl-5654d968df-4xrjc orders] Listening on port 3000, auth
[orders-depl-5654d968df-4xrjc orders] Connected NATS
[orders-depl-5654d968df-4xrjc orders] Connected to Mongodb
[orders-depl-5654d968df-4xrjc orders] Message received: ticket:created / orders-service
[tickets-depl-75d6488d5f-m4j4c tickets] Event published to subject ticket:created
[orders-depl-5654d968df-4xrjc orders] Event published to subject order:created
[tickets-depl-75d6488d5f-m4j4c tickets] Message received: order:created / tickets-service
[expiration-depl-645b8db6f8-kb8jb expiration] Message received: order:created / expiration-service
[orders-depl-5654d968df-4xrjc orders] Message received: ticket:updated / orders-service
[expiration-depl-645b8db6f8-kb8jb expiration] Waiting this many milliseconds to process the job:  59946
[tickets-depl-75d6488d5f-m4j4c tickets] Event published to subject ticket:updated
[expiration-depl-645b8db6f8-kb8jb expiration] Event published to subject expiration:complete


TEST
make new ticket
POST the ticket id on /api/orders
[orders-depl-846fcc5b99-6tj4p orders] Message received: expiration:complete / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:cancelled / tickets-service
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:cancelled
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:created
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:created / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:created / tickets-service
[expiration-depl-79b58d884f-bxcsq expiration] Message received: order:created / expiration-service
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:created
[orders-depl-846fcc5b99-6tj4p orders] Message received: expiration:complete / orders-service
[expiration-depl-79b58d884f-bxcsq expiration] Waiting this many milliseconds to process the job:  59992
[expiration-depl-79b58d884f-bxcsq expiration] Event published to subject expiration:complete
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:cancelled / tickets-service
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:cancelled
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/payments$ docker build -t iceshop/payments .
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/payments$ docker push iceshop/payments

iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing/payments$ npm install mongoose-update-if-current


[payments-depl-7887686465-k8n7l payments] Using ts-node version 8.10.2, typescript version 3.9.7
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:created
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:created / tickets-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[expiration-depl-79b58d884f-bxcsq expiration] Message received: order:created / expiration-service
[expiration-depl-79b58d884f-bxcsq expiration] Waiting this many milliseconds to process the job:  59911
[expiration-depl-79b58d884f-bxcsq expiration] Event published to subject expiration:complete
[orders-depl-846fcc5b99-6tj4p orders] Message received: expiration:complete / orders-service
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:cancelled
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:cancelled / tickets-service
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated
[payments-depl-7887686465-k8n7l payments] Listening on port 3000, payment
[payments-depl-7887686465-k8n7l payments] Connected NATS
[payments-depl-7887686465-k8n7l payments] Message received: order:created / payments-service
[payments-depl-7887686465-k8n7l payments] Message received: order:created / payments-service
[payments-depl-7887686465-k8n7l payments] Message received: order:created / payments-service
[payments-depl-7887686465-k8n7l payments] Message received: order:cancelled / payments-service
[payments-depl-7887686465-k8n7l payments] Message received: order:cancelled / payments-service
[payments-depl-7887686465-k8n7l payments] Message received: order:cancelled / payments-service


npm install stripe

kubectl create secret generic stripe-secret --from-literal STRIPE_KEY=<stripe_secret_key>
kubectl get secrets

Manual test:
GET https://ticketing.dev/api/users/currentuser
POST https://ticketing.dev/api/tickets
{
    "title": "concert",
    "price": 911
}
POST https://ticketing.dev/api/orders
{
    "ticketId": "5f26bf120b2673001f533697"
}
POST https://ticketing.dev/api/payments
{
	"orderId": "5f26bf2357fb940018d96811",
	"token": "tok_visa"
}
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:created / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:created
[orders-depl-846fcc5b99-6tj4p orders] Event published to subject order:created
[tickets-depl-74957d868b-cp6j7 tickets] Message received: order:created / tickets-service
[payments-depl-5577446cdd-sp2ts payments] Message received: order:created / payments-service
[orders-depl-846fcc5b99-6tj4p orders] Message received: ticket:updated / orders-service
[tickets-depl-74957d868b-cp6j7 tickets] Event published to subject ticket:updated
[expiration-depl-5987b9d7f-k9ww5 expiration] Connected to NATS
[expiration-depl-5987b9d7f-k9ww5 expiration] Message received: order:created / expiration-service
[expiration-depl-5987b9d7f-k9ww5 expiration] Waiting this many milliseconds to process the job:  59192


service info:
iceshop@iceshop-X580VD:~/Sites/microservicests/ticketing$ kubectl describe service ingress-nginx-controller -n ingress-nginx

USE HTTPS IN TESTING CLIENT
https://ticketing.dev/api/users/currentuser


fake visa credit card
4242424242424242

https://docs.github.com/en/actions


TODO Creating a Hosted Cluster
install https://github.com/digitalocean/doctl
payments$ doctl auth init
payments$ doctl kubenetes cluster kubeconfig save ticketing
payments$ kubectl get nodes
payments$ kubectl config view
payments$ kubectl config use-context <context_name>
payments$ kubectl get nodes
payments$ kubectl get pods

skaffold dev --status-check=false

https://ticketing.dev/

kubectl get namespaces
kubectl get services -n <namespace>
baseURL: 'http://ingress-nginx-controller.ingress-nginx.svc.cluster.local'
baseURL: 'http://<name_of_service>.<namespace>.svc.cluster.local'

install doctl
generate dg token
doctl auth init
doctl kubenetes cluster kubeconfig save pristine-ticketing
kubectl get pods
kubectl get nodes
kubectl config view
find:
- context:
    cluster: do-ams3-pristine-ticketing
    user: do-ams3-pristine-ticketing-admin
  name: do-ams3-pristine-ticketing

kubectl config use-context docker-desktop

create secrets manually in dg
kubectl create secret generic jwt-secret --from-literal=JWT_KEY=50t8yh420598
kubectl create secret generic jwt-secret --from-literal=STRIPE_KEY=
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.43.0/deploy/static/provider/do/deploy.yaml
kubectl get pods


I thought this might help someone to know how the pods/instances can be scaled(increased or decreased) based on demand. This helps when you are running multiple instance of the pods in production and based on the demand you can scale them.
Follow below link to install Kubernetes Dashboard:
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui
Use below guide to create Bearer Token for Service Account and use it to login and monitor and configure cluster:
https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md
I tried installed and configured in my local cluster and it worked.



I solved this problem by adding TLS cert to Nginx-Ingress (as you suggested in video 489).
 1. I followed this guide by DigitalOcean on "How to Set Up an Nginx Ingress with Cert-Manager on DigitalOcean Kubernetes"
 2. Then, changed the cookie-session "secure" prop back to "true"


https://kubernetes.github.io/ingress-nginx/deploy/#quick-start
ingress nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/cloud/deploy.yaml
kubectl get pods --namespace=ingress-nginx


prune docker
https://linuxize.com/post/how-to-remove-docker-images-containers-volumes-and-networks/

docker build -t pristinewebdev/auth .

make sure you're using docker desktop
kubectl config view
kubectl get pods

common errors response structure
{errors: {message: string, field?: string}[]}

auth strategy:
the services should know expiration of jwt token
auth service should publish an event userBanned on all other services' in-memory cache to prevent any window of unexpired token
text message based 2FA implementation:
- When a user attempts to login, generate a random 6-digit number and store it in the auth service's database along the user model
- Emit an event with the users phone number + the 6-digit number
- A new service called 'text service' or something similar should receive that event and send a text to the user with the 6-digit number
- User receives the text and enters the 6-digit number, sending a second request to the auth service
- Auth service make sure the correct 6-digit number was entered. If it was, the user should then be authenticated.

decode the cookie value to see the jwt
imperative command
kubectl create secret generic jwt-secret --from-literal=JWT_KEY=asdf
kubectl get pods
kubectl describe pods <name_of_pod>

docker build -t pristinewebdev/auth .
docker push pristinewebdev/auth
docker build -t pristinewebdev/client .
docker push pristinewebdev/client
docker build -t pristinewebdev/tickets .
docker push pristinewebdev/tickets

cross namespace communication:
kubectl get namespace
kubectl get services
kubectl get services -n ingress-nginx
http://nameofservice.namespace.svc.cluster.local
http://ingress-nginx.ingress-nginx.svc.cluster.local

npm update @pristinetickets/common


access the shell inside a pod
kubectl get pods
kubectl exec -it <pod_name> sh

port forwarding in k8s
kubectl get pods
kubectl port-forward <pod_name> 4222:4222
then:
nats-test git:(dev) ✗ npm run publish
nats-test git:(dev) ✗ npm run listen
refresh rs

monitoring
localhost:8222/streaming
http://localhost:8222/streaming/channelsz?subs=1


graceful shutdown
process.on('SIGINT', () => { stan.close() });
process.on('SIGTERM', () => { stan.close() });


Core Concurrency Issues:
- Listener can fail to process the event
- One listener might run more quickly than the other
- NATS might think a client is still alive when it is dead
- We might receive the same event twice


Solution:
that won't work - run one copy of Account srv
that won't work - figure out every possible error case and write code to handle it
that won't work - share state between services of last event processed
that won't work - last event processed tracked by resource id
listener will be the one to set the seqId and lastSeqId
Understand the service where it even comes from then add last_transaction_id field will solve concurrency issues.
Per-user lock out.
